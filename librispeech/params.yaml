

out_dir: ./exp
# device: 'cpu'
device: 'cuda:0'

####################### dataset ###########################
dataset_dir: /ceph-fj/data/librispeech/LibriSpeech
test_splits: ["test-clean", "test-other"]
# select_n_sentences: [10, 10]

test_csv:
    - !ref <out_dir>/test-clean.csv
    - !ref <out_dir>/test-other.csv

batch_size: 10

####################### Features ###########################

sample_rate: 16000
n_fft: 400
n_mels: 80

compute_features: !new:speechbrain.lobes.features.Fbank
    sample_rate: !ref <sample_rate>
    n_fft: !ref <n_fft>
    n_mels: !ref <n_mels>

####################### Model parameters ###########################
# Transformer
d_model: 768
nhead: 8
num_encoder_layers: 12
num_decoder_layers: 6
d_ffn: 3072
transformer_dropout: 0.0
activation: !name:torch.nn.GELU
output_neurons: 5000
vocab_size: 5000

# Outputs
blank_index: 0
label_smoothing: 0.1
pad_index: 0
bos_index: 1
eos_index: 2
unk_index: 0

# speechbrain decoding parameters
min_decode_ratio: 0.0
max_decode_ratio: 1.0
valid_search_interval: 10
valid_beam_size: 10
test_beam_size: 66
lm_weight: 0.60
ctc_weight_decode: 0.40

############################## Models ################################

CNN: !new:speechbrain.lobes.models.convolution.ConvolutionFrontEnd
    input_shape: (8, 10, 80)
    num_blocks: 3
    num_layers_per_block: 1
    out_channels: (128, 256, 512)
    kernel_sizes: (3, 3, 1)
    strides: (2, 2, 1)
    residuals: (False, False, False)

Transformer: !new:speechbrain.lobes.models.transformer.TransformerASR.TransformerASR # yamllint disable-line rule:line-length
    input_size: 10240
    tgt_vocab: !ref <output_neurons>
    d_model: !ref <d_model>
    nhead: !ref <nhead>
    num_encoder_layers: !ref <num_encoder_layers>
    num_decoder_layers: !ref <num_decoder_layers>
    d_ffn: !ref <d_ffn>
    dropout: !ref <transformer_dropout>
    activation: !ref <activation>
    normalize_before: False

ctc_lin: !new:speechbrain.nnet.linear.Linear
    input_size: !ref <d_model>
    n_neurons: !ref <output_neurons>

seq_lin: !new:speechbrain.nnet.linear.Linear
    input_size: !ref <d_model>
    n_neurons: !ref <output_neurons>

log_softmax: !new:torch.nn.LogSoftmax
    dim: -1

model: !new:torch.nn.ModuleList
    - [!ref <CNN>, !ref <Transformer>, !ref <seq_lin>, !ref <ctc_lin>]

modules:
    CNN: !ref <CNN>
    Transformer: !ref <Transformer>
    seq_lin: !ref <seq_lin>
    ctc_lin: !ref <ctc_lin>

# differences between model and modules:
#  model is for recovering from checkpoints
#  modules is for neural network computation

normalize: !new:speechbrain.processing.features.InputNormalization
    norm_type: global
    update_until_epoch: 4


tokenizer: !new:sentencepiece.SentencePieceProcessor

pretrained_lm_tokenizer_path: speechbrain/asr-transformer-transformerlm-librispeech

pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer
    collect_in: !ref <out_dir>
    loadables:
        tokenizer: !ref <tokenizer>
        model: !ref <model>
    paths:
        tokenizer: !ref <pretrained_lm_tokenizer_path>/tokenizer.ckpt
        model: !ref <pretrained_lm_tokenizer_path>/asr.ckpt

error_rate_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats
